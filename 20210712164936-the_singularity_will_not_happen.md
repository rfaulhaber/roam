# the singularity will not happen

The core assumptions that the concept of the [[singularity]] or &ldquo;superintelligence&rdquo; rests upon are the following:

1.  Technological progress is always improving
2.  There is a tendency to automate labor over time
3.  [[Artificial intelligence]] will get to a sufficiently intelligent point that it can do not only one task better than humans, but _all_ tasks

There are a number of problems with this:

1.  [[Automation]] is about reducing [[necessary labor-time]]. In this way, the argument still holds, but falls apart when we look out at the world. As of writing this, there are no firms who are actively looking to create a general artificial intelligence
2.  Intelligence is situational. Intelligence comes from one&rsquo;s environment and the problems that arise in said environment
    1.  As the article linked to below says, you could not simply put a human brain in an octopus&rsquo;s and assume it&rsquo;ll be able to survive its environment. Much of what makes a human human is hard-coded (but not everything!)
3.  There is no such thing as &ldquo;general&rdquo; intelligence
4.  This puts far too much faith in software developers


<a id="orgd924d96"></a>

## Links

-   [The implausibility of intelligence explosion | by Fran√ßois Chollet | Medium](https://medium.com/@francois.chollet/the-impossibility-of-intelligence-explosion-5be4a9eda6ec)


<a id="org0607228"></a>

## Backlinks

-   [[the singularity will not happen]]
